1. Attention: 注意某些重要的词或者说需要关注的词

2. 传统的word2vec的问题: 

    不同的词在不同的语境中表达的意思不一定相同,但是转变为vector之后,就不变了

    尝试想法:如果一个词能加入context,那么同一个词在不同的context中就有了不同的vector表达

    bert就是做这个事情

3. 自注意力机制: 比如有一个句子，我们对里面的某一个词进行编码，引入self-attention之后，这个词就要考虑到该句子中其他词的因素，既要考虑上下文语境，而不是单单对这个词进行编码

4. self-attention计算方法：QKV矩阵 

5. Mutil-headed机制：一组QKV等得到一个词的encode，bert通常做8组，然后拼接在一起，再做降维得到最后的encode

6. 问题提出

    - 对于一个序列，同一个词在不同位置似乎表达相同，于是需要对位置信息进行表达，在对词进行初步encode之后加上位置信息即可 x + t
    - 在堆叠很多层之后，不能完全保证效果就比使用self-attention之前好，于是做一个同等映射，时结果至少不比word2vec差



$n^2$

